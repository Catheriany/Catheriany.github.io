
在深度学习的浪潮中，模型规模的不断扩大带来了前所未有的挑战。从 GPT-2 到 GPT-3，再到更庞大的模型，参数数量从数亿跃升至数千亿甚至万亿级别。这种增长不仅对计算资源提出了更高的要求，也对内存管理、通信效率和训练稳定性带来了严峻考验。从下面的图1可以看出近年来模型大小的上限正在逐步增大，高性能模型中绝大多数都是几十亿和上百亿的模型。

![图1：LLM MMLU性能以及大小的时间轴（来源：Information is Beautiful）](https://informationisbeautiful.net/visualizations/the-rise-of-generative-ai-large-language-models-llms-like-chatgpt)

在这样的背景下，DeepSpeed 在 2020 年应运而生，成为解决大模型训练难题的关键工具。DeepSpeed 是微软开发的开源深度学习优化库，旨在加速大规模模型的训练和推理过程。它在 PyTorch 的框架上构建，提供了一系列先进的技术，以支持具有数十亿甚至数万亿参数的模型训练。

## 第一关：内存不足

算力的不足会显著增加训练所需的时间，但随着模型规模的扩大，内存容量已无法容纳整个模型的参数，从而导致许多模型无法在单个 GPU 上进行训练。到了这里，有些人可能会想到目前训练模型都不是靠单个 GPU，而是有很多 GPU 甚至是多个集群进行分布式训练。有了多个 GPU，这个问题不就能自然地被缓解了吗？但在传统分布式训练中，每个 GPU 都需要完整存储模型参数、梯度和优化器状态这三座"大山"。以 GPT-3 为例，仅保存 Adam 优化器的动量和方差就需要约20倍参数量的存储空间。因此，仅靠这种方式的分布式训练是无法有效缓解内存不足的问题的。但在分布式训练的基础上，DeepSpeed 的 ZeRO（零冗余优化器）技术通过精密的内存分配策略，将这三部分数据分解并重新优化存储。

ZeRO，即 Zero Redundancy Optimizer，是 DeepSpeed 原创的一种在数据和模型并行训练中消除内存冗余的内存优化方法。其分为两套优化，分别为 ZeRO-DP 和 ZeRO-R。为了能够更加直观的看出 ZeRO 对内存方面带来的优化，我们假设一个情景：在不使用 ZeRO 的情况下，我们使用 64 个 GPU 来训练一个使用 Adam 优化器的 7.5B 的模型。在这个情况下需要消耗的内存为：



其中 K 为优化器的内存倍数，$$\Psi$$为模型大小，即参数数量。此处的前两个 2 分别为 2 字节的权重与梯度，$$K=12$$（4 字节权重 + 4 字节动量 + 4 字节方差），$$\Psi=7.5B$$，计算出来为 120 GB。我们将此作为基准。

ZeRO-DP 在数据并行的维度上施展魔法。第一阶段是优化器状态分区（Optimizer State Partitioning）。其将优化器状态（如 Adam 的动量、方差）分布到 64 个 GPU 上，每个设备只需保管 1/64 的碎片，显存占用减少 4 倍，通信量与传统数据并行（DP） 相同。此时的消耗内存为：



$$N_d$$为 DP 数，也就是这里的 64。计算出来约为 31.4 GB。

第二阶段继续拆分梯度数据（Gradient Partitioning），梯度数据按需分配到不同 GPU，显存占用进一步减少 8 倍，通信量与传统 DP 相同，适合较大 batch size 和单节点多卡训练 。此时的消耗内存为：


计算出来约为 16.6 GB。

第三阶段更是将参数本身动态分配到各个节点（Parameter Partitioning）。参数仅在计算时加载所需部分，显存占用降低 64 倍，通信量比传统 DP 增加 1.5 倍，更适合支持超大模型和跨节点分布式训练。此时的消耗内存为：


计算出来约为 1.875 GB。

经过这三重解构，显存占用从原始的 120GB 骤降至 1.875 GB，相当于在 64 卡集群中，每张显卡的负担减轻了98%。这种"分而治之"的智慧，让普通显卡也能承载千亿参数的训练任务。例如在训练 1750 亿参数的 GPT-3 时，DeepSpeed 只需 256 块 V100 显卡，而传统方法需要超过 3000 块。

于此同时，DeepSpeed 还将 ZeRO 非常好地应用到了实现 3D parallelism 当中来进一步降低显存占用并提高计算效率。3D Parallelism（3D并行）是一种同时结合了数据并行（Data Parallelism, DP）、流水线并行（Pipeline Parallelism, PP）和张量并行（Tensor Parallelism, TP）的混合分布式训练技术。其核心思想是通过多维度的任务划分，最大化硬件资源利用率，同时降低单设备负载。传统的数据并行如同流水线上的复制工人，每个工位重复相同工序。3D并行架构则构建起立体化的协作网络：数据并行处理不同样本，张量并行拆分矩阵运算，流水线并行划分网络层。

当模型突破千亿参数时，即便是使用 3D 并行的显存优化方式也已触及天花板。DeepSpeed 在原有 ZeRO 技术（ZeRO3）上进行拓展，祭出了主要围绕异构内存的技术 ZeRO-Infinity，将内存版图扩展到 CPU 和 NVMe 硬盘。ZeRO-Infinity 中的 Infinity Offload Engine 能自动识别参数访问频率，高频数据驻留 GPU 显存，低频数据迁移至 CPU 内存，历史数据沉淀到 NVMe 存储。配合将一个大算子分成多个小块逐一加载计算的内存分块技术（Memory-centric tiling）等，单个 GPU 可驾驭远超自身容量的模型参数。这就像为每个 GPU 配备了三层存储阁楼，需要时随时存取，无需时刻背负全部家当。在微软的实验中，这种方法成功训练出1.6万亿参数的模型，其显存占用仅相当于传统方法的1/20。

## 第二关：通信效率

在传统的大模型分布式训练中，通信开销往往成为制约性能的关键因素。当模型参数分布在数百个 GPU 上时，设备间的数据交换会产生巨大的带宽压力。从上述技术可以看出，DeepSpeed 的诸多降低内存占用的方法使得原本分布式训练中十分关键的通信带宽/效率有了更大的压力。针对这一痛点，DeepSpeed 开发了多层次的通信优化方案，构建起一套完整的效率提升体系。

在 ZeRO 的基础上，DeepSpeed 增加了 ZeRO++ 的通信优化。其主要针对大模型的训练。量化通信技术（qwZ）是 ZeRO++ 通信优化的第一招。该技术创造性地在 All-Gather 操作前后引入 FP16 与 INT8 的实时转换。不同于简单的全局量化，DeepSpeed 采用了精细化的 block-based 量化策略：将权重矩阵划分为多个子块，对每个子块独立计算量化参数，最大程度保留有效信息。

分级权重分割（hpZ）则是在每个计算节点内部维护完整的模型副本，通过智能的内存-通信权衡策略，将反向传播时的 All-Gather 操作转化为节点内通信。具体实现上，前向传播时各节点保持参数一致性，反向传播时仅需在节点间交换梯度更新量。

量化梯度通信（qgZ） 进一步将优化延伸至梯度同步环节。DeepSpeed 设计了三段式处理流程：首先对梯度张量进行结构化分割和重排，确保量化后的数据分布均衡；然后在节点内通过 All-to-All 操作重新分配数据块，配合本地归约计算；最后在节点间执行二次 All-to-All 和 全局归约。

除开 ZeRO++, DeepSpeed 还在常用一些优化器上做了特殊的优化。比如，DeepSpeed 开发了 1-bit Adam 优化器来减少 Adam 优化器的通信开销。这种两阶段算法在预热阶段采用常规 Adam 更新，当梯度方差低于设定阈值后自动切换至压缩模式：保持方差参数不变，对一阶矩估计进行 1-bit 量化。令人惊叹的是，该算法仍能保持与普通 Adam 相同的收敛性。

在处理超长文本序列时，常规注意力机制的计算量呈平方级增长。DeepSpeed Ulysses 创新性地将序列拆分为片段分发给不同 GPU，通过智能通信重组注意力头计算（即 Sequence Parallel）。这种设计拥有更小的累积通信量 O(N/P)，而非普通情况的 O(N)。这种技术让训练超过百万个 token 的序列成为可能，为长文档理解、基因组分析等任务创造可能。就像多位翻译专家分工处理文章段落，最后汇总出完整译文。

## 第三关：训练效率与稳定性

更大的模型规模同时还会对训练的效率和稳定性带来更大的挑战。为此，DeepSpeed 开发了 Data Efficiency Library，其中主要的两项核心技术为：Curriculum Learning（课程学习）和 Random Layerwise Token Dropping（随机分层令牌丢弃，简称random-LTD）。这两种技术分别从数据采样和计算路由的角度提升训练效率，同时保持模型质量。

Curriculum Learning 模拟人类循序渐进的学习过程，初期用“简单”的训练样本，之后逐渐提高样本“难度”，难度上限和下限由用户给定。“难度”的定义可以有多种，但在 DeepSpeed 中，curriculum learning 主要针对 GPT 类模型，难度的定义为 sequence length。Sequence length 越长则难度越高。这种技巧被他们称作 sequence length warmup。也就是说，初期用短文本训练模型，逐步增加序列长度。这种"先学走再学跑"的策略，使模型在提升 batch size 和学习率时仍保持稳定，并且该方法只影响数据并行，所以与其他很多优化正交（例如 ZeRO，ZeRO-Offload，3D Parallelism）。在 GPT-3 和 BERT 预训练中，使用该方法可以最高实现 1.5 倍的数据节省。

Random Layerwise Token Dropping 技术则是随机舍弃每层中一些 tokens，在 transformer 层输出后再将舍弃掉的 tokens 加上恢复完整的序列。这种方法能在正确率基本不变的情况下提高训练效率/降低训练时间。与其他 token 舍弃方法不同，该方法不需要重要性的衡量指标、不需要任何特殊 token 处理（例如 [CLS]）同时除了第一层和最后一层，中间层仍收到完整的序列输入。

除开上述介绍的优化技术外，DeepSpeed 还有很多其他的优化方法，例如更加优化的稀疏注意力内核，对 MOE 的并行组合模式等。目前 （此时成文时间）DeepSpeed 的 GitHub 仓库有二十万行以上的代码。DeepSpeed 通过内存优化（ZeRO）、通信加速（量化/序列并行）、训练策略（课程学习/LTD）为主的三重创新，将大模型训练的门槛从“千卡集群”降至“百卡级”，同时保持高效与稳定，这让训练更大规模的模型成为了可能。此外，DeepSpeed 的 ZeRO 技术已然成为了分布式训练以及训练框架中不可或缺的内存优化技巧。

#### 推荐阅读

- [https://www.deepspeed.ai](https://www.deepspeed.ai/)
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
- [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/pdf/2104.07857)
